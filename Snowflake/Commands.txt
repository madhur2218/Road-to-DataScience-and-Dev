use role accountadmin;
use warehouse compute_wh;


create or replace database mydb;
create or replace schema myschema;

USE SCHEMA MYDB.MYSCHEMA

//Create a permanant table, by default this wull crete a permanent table
CREATE OR REPLACE TABLE PERMANENT_TABLE(
    ID INT,
    NAME STRING
);

//VIEWING RESULTS FROM PERMANENT_TABLE 
SELECT * FROM PERMANENT_TABLE 

//CREATING A TRANSIENT TABLE 
CREATE OR REPLACE TRANSIENT TABLE TRANSIENT_TABLE(
    ID INT,
    NAME STRING
);

//CREATING A TEMPORARY TABLE 
CREATE OR REPLACE TEMPORARY TABLE TEMPORARY_TABLE(
    ID INT,
    NAME STRING
);


SHOW TABLES

ALTER TABLE PERMANENT_TABLE SET DATA_RETENTION_TIME_IN_DAYS  = 90; 
SHOW TABLES


ALTER TABLE TRANSIENT_TABLE SET DATA_RETENTION_TIME_IN_DAYS  = 2; 
ALTER TABLE TEMPORARY_TABLE SET DATA_RETENTION_TIME_IN_DAYS  = 2; 

//different types of tables in snowflake

//views in snowflake

//Set the roles, warehouses and databases in snowflake
CREATE OR REPLACE TABLE employees(
    id integer,
    name varchar(50),
    department varchar(50),
    salary integer

);

select * from employees

//INSERTING INTO EMPLOYEES TABLE 
INSERT INTO employees (id, name , department, salary)
values
    (1,'Pat Fay', 'HR', 50000),
    (2,'Donald OConnel', 'IT', 60000),
    (3,'Steven King', 'Sales', 70000),
    (4,'Susan Marvis', 'IT', 50000),
    (4,'Jeniffer Whalen', 'Marketing', 60000);

SELECT * FROM EMPLOYEES

//LETS CREATE A VIEW CALLED "IT_EMPLOYEES" THAT ONLY INCLUDES THE EMPLOYEES FROM IT DEPT

CREATE OR REPLACE VIEW it_employees AS 
SELECT * FROM EMPLOYEES
WHERE DEPARTMENT = 'IT';

SELECT * FROM it_employees

//LETS CREATE A VIEW CALLED "IT_EMPLOYEES" THAT ONLY INCLUDES THE EMPLOYEES FROM HR DEPT
CREATE OR REPLACE SECURE VIEW hr_employees AS 
SELECT * FROM EMPLOYEES
WHERE DEPARTMENT = 'HR';

SELECT * FROM hr_employees

//CREATING A STANDARD VIEW THAT AGGREGATES THE SALARIES BY DEPT
CREATE OR REPLACE VIEW employee_salaries
AS SELECT department, sum(salary) as total_salary
from employees
group by department;

select * from employee_salaries

//Creating a materialized view that aggregates the salaries by department 

CREATE OR REPLACE MATERIALIZED VIEW materialized_employee_salaries
AS SELECT department, sum(salary) as total_salary
from employees
group by department;

select * from materialized_employee_salaries  //(this will bring the data from the cache)


//Stages in snowflake - table stage, user stage and named stage

CREATE OR REPLACE TABLE CUSTOMER(
    id integer,
    name varchar(50),
    age integer,
    state varchar(50)
)

//Access table stage 
list @%customer

//Access named stage
list @~;

//Create a named stage 
list @CUSTOMER_STAGE;

//Truncate the table CUSTOMER
TRUNCATE TABLE CUSTOMER

//CRETE A NAMED STAGE
CREATE OR REPLACE STAGE CUSTOMER_STAGE

//LOAD DATA TO CUSTOMER TABLE 
copy into customer 
from @CUSTOMER_STAGE
file_format = (TYPE = 'CSV' SKIP_HEADER = 1);


select * from titanic_dataset
where name = 'Braund, Mr. Owen Harris'

//use UI in order to create STAGES (internal and external) in snowflake 


//File format
CREATE OR REPLACE TABLE STUDENT (
    id integer,
    name varchar(50),
    age integer,
    marks integer
)


//Creating a named stage
CREATE OR REPLACE STAGE STUDENT_STAGE

//ACCESS THE NAMES INTERNAL STAGE
LIST @STUDENT_STAGE

//Load data to customer table without file format 
COPY INTO STUDENT
FROM @STUDENT_STAGE
file_format = (TYPE = 'CSV' SKIP_HEADER = 1);

SELECT * FROM STUDENT

//TRUNCATE TABLE 
TRUNCATE TABLE STUDENT

//CREATE A CSV FILE FORMAT
CREATE OR REPLACE FILE FORMAT CSV_FORMAT
TYPE = 'CSV',
FIELD_DELIMITER = ','
RECORD_DELIMITER = '\n'
SKIP_HEADER = 1;

//Load data to customer table with file format
COPY INTO STUDENT
FROM @STUDENT_STAGE
FILE_FORMAT = (FORMAT_NAME = CSV_FORMAT)

SELECT * FROM STUDENT

//create json file format
CREATE OR REPLACE  FILE FORMAT JSON_FORMAT
TYPE = 'JSON';

//Different data loading approaches in snowflake 
--bulk(using copy command) and continuous load (using snowpipe)

--BULK LOADING -> TRANSFER DATA TO INTERNAL STAGES OF SNOWFLAKE -> COPY STAGE DATA INTO DB 
--EXTERNAL STAGE (S3,AZURE BLOB) -> COPY INTO DB 


//Continuous data loading(snowpipe)

//LOADING DATA FROM EXTERNAL STORAGE - S3

USE ROLE ACCOUNTADMIN;
USE WAREHOUSE COMPUTE_WH;
USE SCHEMA MYDB.MYSCHEMA;

//CREATE USER TABLE 
CREATE OR REPLACE TABLE USER(
    id integer,
    name varchar(100),
    location varchar(50),
    email varchar(50)
);

//creating a storage integration with s3 and iam role
CREATE OR REPLACE STORAGE INTEGRATION s3_int
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = 'S3'
ENABLED = TRUE
STORAGE_AWS_ROLE_ARN = 'ENTER ARN VALUE FROM AWS'
STORAGE_ALLOWED_LOCATIONS= 'ENTER NAME OF BUCKET HERE'

//DESCRIBING STORAGE INTEGRATION
DESC INTEGRATION s3_int;

//Continuous Data loading(need to go through this topic)


//Snowflake streams (tracks all the DML operations against the source table)

//creating a source table
CREATE OR REPLACE TABLE source_table1(
    id int,
    name varchar,
    created_date date
);

insert into source_table1 values
    (1, 'madhur', '2023-12-11'),
    (2, 'naman', '2022-12-11'),
    (3, 'ashutosh', '2024-12-11')

//Creating a standard stream on the table 
CREATE OR REPLACE STREAM standard_stream ON TABLE source_table1

insert into 
values
    (4, 'ishant', '2023-12-11')
    
select * from standard_stream

//delete data from table 
delete from source_table1
where id =4

update source_table1 set name = 'Elon' 
where id =1

//APPEND ONLY STREAM

CREATE OR REPLACE STREAM applend_only_stream ON TABLE source_table2 APPEND_ONLY = TRUE;

select * from applend_only_stream

insert into source_table2
values
    (1, 'ishant', '2023-12-11')

select * from applend_only_stream

update source_table1 set name = 'Elon' 
where id =1

//HOW TO USE STREAM IN ETL PROCESS

CREATE OR REPLACE TABLE target_table2(
    id int,
    name varchar,
    created_date date
)

select * from standard_stream

insert into target_table2
select id, name,created_date from append_only_stream;


//INSERT ONLY STREAM

CREATE EXTERNAL TABLE ext_table
LOCATION = @MY_AWS_STAGE
FILE_FORMAT = file_format

create stream my_External_stream
on external table ext_table
insert_only = true


//SNOWFLAKE TASKS (USER MANAGED TASK AND SERVERLESS SNOWFLAKE TASK)


//without task 

CREATE OR REPLACE TABLE SOURCE_TABLE (
    id int,
    name varchar,
    created_date date
)

//insert some rows in source_table
insert into SOURCE_TABLE
values
    (1, 'ishant', '2023-12-11')

//create a target table
CREATE OR REPLACE TABLE TARGET_TABLE(
    id int,
    name varchar,
    crated_date date,
    created_day varchar,
    created_month varchar,
    created_year varchar

)

INSERT INTO TARGET_TABLE
SELECT
    a.id,
    a.name,
    a.created_date,
    DAY(a.created_date) AS created_day,
    MONTH(a.created_date) AS created_month,
    YEAR(a.created_date) AS created_year
FROM SOURCE_TABLE a
LEFT JOIN target_table b
    ON a.id = b.id
WHERE b.id IS NULL;


insert into SOURCE_TABLE
values
    (3, 'MALHAR', '2023-12-11')


//inserting data into target table through task 
//(THIS TASK WILL RUN AFTER EVERY 5 MIN AND WILL INSERT INTO THE TARGET TABLE FROM THE SOURCE TABLE)
CREATE OR REPLACE TASK my_task
WAREHOUSE = COMPUTE_WH
SCHEDULE = '5 MINUTE'
AS
INSERT INTO TARGET_TABLE
SELECT
    a.id,
    a.name,
    a.created_date,
    DAY(a.created_date) AS created_day,
    MONTH(a.created_date) AS created_month,
    YEAR(a.created_date) AS created_year
FROM SOURCE_TABLE a
LEFT JOIN target_table b
    ON a.id = b.id
WHERE b.id IS NULL;

//SHOW TASK IN SNOWFLAKE SCHEMA
SHOW TASKS

ALTER TASK my_task RESUME;
ALTER TASK my_task SUSPEND;

//SNOWFLAKE TIME TRAVEL AND FAIL SAFE


















































    

















